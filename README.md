# Washington Post Article Scraper with Comments Extraction

This repository contains two Python scripts designed to scrape article content, including comments, from The Washington Post website. The scripts leverage `Selenium` and `BeautifulSoup` to automate the process of logging in, navigating to articles, extracting content, and retrieving comments. The credentials and other required information are read from a configuration file.

## Prerequisites

- **Python 3.6+**
- **Google Chrome browser**
- **ChromeDriver**
- **Required Python packages** (install using pip):
  ```bash
  pip install requests beautifulsoup4 selenium webdriver-manager
  ```
  Overview
  The project consists of two main scripts:

ScrapWasPostArtclesURL.py: Fetches article URLs from a specified subsection of The Washington Post and saves them into a JSON file.
ScrapWasPostArticlesContent.py: Takes the JSON file generated by the first script, scrapes each article's content (headline, subheading, body text), and fetches comments embedded in an iframe. The extracted data is saved into individual JSON files.
Both scripts read the required inputs (email, password, start date, end date, and subsection) from a config.json file.

Configuration
Create a config.json file in the root directory of your project with the following structure:


{
"email": "your_email@example.com",
"password": "your_password",
}

Explanation of Configuration Fields:
email: Your Washington Post account email.
password: Your Washington Post account password.

Script Details
ScrapWasPostArtclesURL.py
Key Features:

Logs into The Washington Post website.
Fetches URLs of articles published within a specified date range from a selected subsection.
Saves the fetched URLs into a JSON file.
Usage:

Input: The script reads email, password, start_date, end_date, and subsection from the config.json file.
Output: A JSON file containing the URLs of the articles within the specified date range.
Example Usage:
After running the script, it will generate a JSON file containing all the article URLs from the specified subsection and date range.

ScrapWasPostArticlesContent.py
Key Features:

Takes the JSON file of URLs generated by ScrapWasPostArtclesURL.py.
Extracts the article content (headline, subheading, body) from each URL.
Locates and extracts comments from the embedded comments iframe.
Saves the article content and comments into individual JSON files.
Usage:

Input: The script reads email, password, and the JSON file of URLs (generated by the first script).
Output: Individual JSON files for each article containing the headline, subheading, article body, and comments.
Example Usage:
The script processes each URL from the JSON file and generates a separate JSON file with the scraped content and comments.

Directory Structure

.
├── ScrapWasPostArtclesURL.py
├── ScrapWasPostArticlesContent.py
├── config.json
├── URL/  # Folder where the first script stores fetched URLs.
│   └── WP_<subsection>_URL_<date_range>.json
└── Data/  # Folder where the second script stores the article content and comments.
    └── <article_json_files>.json


Steps to Run the Scripts
Step 1: Create a config.json file as described in the Configuration section.

Step 2: Run ScrapWasPostArtclesURL.py to fetch the article URLs from The Washington Post.
This will create a JSON file containing a list of URLs based on the date range and subsection you specify in the config.json file.

Step 3: Run ScrapWasPostArticlesContent.py to scrape the content and comments from the URLs generated in Step 2.
This script will create individual JSON files for each article, containing the headline, subheading, body, and comments.

Notes

Authentication: Both scripts require login credentials to access The Washington Post's content. Ensure you have entered valid email and password details in the config.json file.

Subsections: The first script supports scraping from various subsections like politics, business, world, technology, and more. Make sure to specify a valid subsection in the config.json.

Error Handling: The scripts include error handling for various cases, including missing elements, failed authentication, and more. Invalid URLs or errors during scraping may result in the JSON file being deleted to ensure clean data output.

Customization: You can easily modify the scripts to scrape different subsections or adjust the date range according to your requirements.
